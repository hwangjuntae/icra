#!/usr/bin/env python3

import cv2
import os
import numpy as np
import time
import sys

# EfficientSAM ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/teus/ws/src/risk_nav/EfficientSAM')

# BLIP2 ê´€ë ¨ import
try:
    from transformers import Blip2Processor, Blip2ForConditionalGeneration
    from PIL import Image as PILImage
    import torch
    BLIP2_AVAILABLE = True
    print("âœ… BLIP2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    BLIP2_AVAILABLE = False
    print(f"âŒ BLIP2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")

# EfficientSAM ê´€ë ¨ import
try:
    from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits
    from torchvision import transforms
    import torch
    import zipfile
    EFFICIENT_SAM_AVAILABLE = True
    print("âœ… EfficientSAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    EFFICIENT_SAM_AVAILABLE = False
    print(f"âŒ EfficientSAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")

class ImageProcessor:
    def __init__(self):
        self.image_dir = os.path.join(os.path.dirname(__file__), 'Camera_rgb')
        self.image_path = os.path.join(self.image_dir, 'camera_rgb_latest.jpg')
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.caption_path = os.path.join(self.image_dir, 'latest_caption.txt')
        self.segmented_path = os.path.join(self.image_dir, 'segmented_latest.jpg')
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.blip2_processor = None
        self.blip2_model = None
        self.efficient_sam_model = None
        
        print("ğŸš€ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.init_models()
    
    def init_models(self):
        """BLIP2ê³¼ EfficientSAM ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ“¥ ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        
        # BLIP2 ëª¨ë¸ ì´ˆê¸°í™”
        if BLIP2_AVAILABLE:
            try:
                print("ğŸ”„ BLIP2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self.blip2_processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
                self.blip2_model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b")
                
                # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
                if torch.cuda.is_available():
                    self.blip2_model = self.blip2_model.to("cuda")
                    print("âœ… BLIP2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU)")
                else:
                    print("âœ… BLIP2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU)")
                    
            except Exception as e:
                print(f"âŒ BLIP2 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.blip2_processor = None
                self.blip2_model = None
        
        # EfficientSAM ëª¨ë¸ ì´ˆê¸°í™”
        if EFFICIENT_SAM_AVAILABLE:
            try:
                print("ğŸ”„ EfficientSAM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                
                # EfficientSAM ê²½ë¡œ ì„¤ì •
                efficient_sam_path = "/home/teus/ws/src/risk_nav/EfficientSAM"
                current_dir = os.getcwd()
                os.chdir(efficient_sam_path)
                
                # EfficientSAM-S ëª¨ë¸ ì••ì¶• í•´ì œ (í•„ìš”ì‹œ)
                weights_dir = os.path.join(efficient_sam_path, "weights")
                zip_path = os.path.join(weights_dir, "efficient_sam_vits.pt.zip")
                model_path = os.path.join(weights_dir, "efficient_sam_vits.pt")
                
                if os.path.exists(zip_path) and not os.path.exists(model_path):
                    print("ğŸ“¦ EfficientSAM-S ëª¨ë¸ ì••ì¶• í•´ì œ ì¤‘...")
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(weights_dir)
                
                # EfficientSAM-Ti ëª¨ë¸ ì‚¬ìš© (ë” ë¹ ë¦„)
                self.efficient_sam_model = build_efficient_sam_vitt()
                
                # ì›ë˜ ê²½ë¡œë¡œ ë³µì›
                os.chdir(current_dir)
                
                print("âœ… EfficientSAM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ EfficientSAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.efficient_sam_model = None
    
    def generate_caption(self, cv_image):
        """BLIP2ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìº¡ì…”ë‹"""
        if not BLIP2_AVAILABLE or self.blip2_processor is None:
            return "BLIP2 ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        try:
            print("ğŸ” BLIP2 ìº¡ì…”ë‹ ì¤‘...")
            
            # OpenCV ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            pil_image = PILImage.fromarray(rgb_image)
            
            # BLIP2 ì²˜ë¦¬
            inputs = self.blip2_processor(pil_image, return_tensors="pt")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # ìº¡ì…˜ ìƒì„±
            with torch.no_grad():
                generated_ids = self.blip2_model.generate(**inputs, max_length=50)
                caption = self.blip2_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # ìº¡ì…˜ ì €ì¥
            with open(self.caption_path, 'w', encoding='utf-8') as f:
                f.write(caption)
            
            print(f"ğŸ“ ìº¡ì…˜ ìƒì„± ì™„ë£Œ: {caption}")
            return caption
            
        except Exception as e:
            print(f"âŒ BLIP2 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨"
    
    def segment_image(self, cv_image):
        """EfficientSAMì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        if not EFFICIENT_SAM_AVAILABLE or self.efficient_sam_model is None:
            return None
        
        try:
            print("ğŸ¯ EfficientSAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¤‘...")
            
            # OpenCV ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ í…ì„œë¡œ ë³€í™˜
            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            pil_image = PILImage.fromarray(rgb_image)
            image_tensor = transforms.ToTensor()(pil_image)
            
            # ì´ë¯¸ì§€ ì¤‘ì•™ í¬ì¸íŠ¸ ì„¤ì •
            h, w = cv_image.shape[:2]
            center_x, center_y = w // 2, h // 2
            
            # EfficientSAM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
            input_points = torch.tensor([[[[center_x, center_y]]]])
            input_labels = torch.tensor([[[1]]])
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            with torch.no_grad():
                predicted_logits, predicted_iou = self.efficient_sam_model(
                    image_tensor[None, ...],
                    input_points,
                    input_labels,
                )
            
            # ê°€ì¥ ì¢‹ì€ ë§ˆìŠ¤í¬ ì„ íƒ
            sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)
            predicted_logits = torch.take_along_dim(
                predicted_logits, sorted_ids[..., None, None], dim=2
            )
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì‹œê°í™”
            segmented_image = cv_image.copy()
            
            # ë§ˆìŠ¤í¬ ì ìš© (ì´ˆë¡ìƒ‰ ì˜¤ë²„ë ˆì´)
            overlay = segmented_image.copy()
            overlay[mask] = [0, 255, 0]  # ì´ˆë¡ìƒ‰
            segmented_image = cv2.addWeighted(segmented_image, 0.7, overlay, 0.3, 0)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€ ì €ì¥
            cv2.imwrite(self.segmented_path, segmented_image)
            
            print("ğŸ¨ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ")
            return segmented_image
            
        except Exception as e:
            print(f"âŒ EfficientSAM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def process_image(self):
        """ì €ì¥ëœ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        if not os.path.exists(self.image_path):
            print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.image_path}")
            return
        
        print(f"ğŸ“¸ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘: {self.image_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        cv_image = cv2.imread(self.image_path)
        if cv_image is None:
            print("âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {cv_image.shape}")
        
        # ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        
        # BLIP2ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
        caption = self.generate_caption(cv_image)
        
        # EfficientSAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        segmented_image = self.segment_image(cv_image)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        process_time = time.time() - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ")
        print(f"ğŸ“ ìº¡ì…˜: {caption}")
        print(f"ğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜: {'ì™„ë£Œ' if segmented_image is not None else 'ì‹¤íŒ¨'}")
        print("="*50)
        
        # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì¶œë ¥
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤:")
        print(f"   - ì›ë³¸ ì´ë¯¸ì§€: {self.image_path}")
        print(f"   - ìº¡ì…˜: {self.caption_path}")
        print(f"   - ì„¸ê·¸ë©˜í…Œì´ì…˜: {self.segmented_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = ImageProcessor()
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤í–‰
    processor.process_image()
    
    print("\nâœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == '__main__':
    main() 